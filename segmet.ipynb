{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc0313c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae58368d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46b9db34",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"D:\\Courses\\CE 7090 -Statistical and Econometric Methods in Civil Engineering II\\HomeWork\\HW-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "351747da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('HW4 Data.txt', delim_whitespace=True, header=None, na_values=-999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bea47ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(999, 49)\n",
      "   3   24  33\n",
      "0   0  63  60\n",
      "1   0   0  60\n",
      "2   0  16  55\n",
      "3   1  30  60\n",
      "4   1  53  60\n"
     ]
    }
   ],
   "source": [
    "# Basic inspection\n",
    "print(df.shape)        # should be (999, 49)\n",
    "print(df[[3,24,33]].head())  # peek at severity (X4), age (X25), speed limit (X34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2beeda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21    879\n",
      "26     27\n",
      "27     27\n",
      "29     12\n",
      "37    619\n",
      "39    180\n",
      "40    180\n",
      "41    180\n",
      "42    180\n",
      "43    180\n",
      "44    180\n",
      "45    916\n",
      "46    715\n",
      "47    715\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count missing values per column\n",
    "missing_counts = df.isna().sum()\n",
    "print(missing_counts[missing_counts > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52547b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(982, 44)\n"
     ]
    }
   ],
   "source": [
    "# Drop columns with > 500 missing values (arbitrary threshold, e.g., X22, X46, X47, X48)\n",
    "cols_to_drop = [col for col in df.columns if df[col].isna().sum() > 500]\n",
    "df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "# Drop rows where age (X25) is missing or zero (assuming 0 means unknown age)\n",
    "df = df[df[24] != 0]  # X25 (driver age) is at index 24\n",
    "\n",
    "# Verify remaining shape after drops\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "461163ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\statsmodels\\base\\optimizer.py:18: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method bfgs is: gtol, norm, epsilon. The list of unsupported keyword arguments passed include: weights. After release 0.14, this will raise.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Log-likelihood = -1830.6116, ll_change = inf, param_change = 0.000000\n",
      "Iteration 1: Log-likelihood = -1830.6116, ll_change = 0.000000, param_change = 0.000000\n",
      "Converged at iteration 1\n",
      "\n",
      "Final Mixing Proportions (Segment-Based): [0.5 0.5]\n",
      "\n",
      "Class 1 Coefficients:\n",
      "adverse_weather   -0.273485\n",
      "dark              -0.047624\n",
      "driver_age        -0.002171\n",
      "male              -0.065744\n",
      "ejected            2.884690\n",
      "no_restraint       0.774311\n",
      "dtype: float64\n",
      "Class 1 Thresholds:\n",
      "0/1    0.295718\n",
      "1/2    0.320259\n",
      "2/3   -0.380990\n",
      "dtype: float64\n",
      "\n",
      "Class 2 Coefficients:\n",
      "adverse_weather   -0.273485\n",
      "dark              -0.047624\n",
      "driver_age        -0.002171\n",
      "male              -0.065744\n",
      "ejected            2.884690\n",
      "no_restraint       0.774311\n",
      "dtype: float64\n",
      "Class 2 Thresholds:\n",
      "0/1    0.295718\n",
      "1/2    0.320259\n",
      "2/3   -0.380990\n",
      "dtype: float64\n",
      "\n",
      "Distribution of Observations by Predicted Segment Class:\n",
      "predicted_class\n",
      "1    982\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.miscmodels.ordinal_model import OrderedModel\n",
    "\n",
    "# -------------------------------\n",
    "# Step 1: Data Preparation\n",
    "# -------------------------------\n",
    "# Load data as before, and create your new predictors.\n",
    "# (For brevity, we reuse the alternative variables code from the previous example.)\n",
    "# df = pd.read_csv('HW4 Data.txt', delim_whitespace=True, header=None, na_values=-999)\n",
    "\n",
    "# Create outcome and some alternative predictors.\n",
    "# Outcome: Injury severity (X4 is column 3, zero-indexed)\n",
    "df['severity'] = df[3]\n",
    "# Let’s assume we want to use these predictors (example alternatives):\n",
    "df['adverse_weather'] = (df[19] != 1).astype(int)  # X20: adverse weather if not 1\n",
    "df['dark'] = df[20].apply(lambda x: 1 if x in [4, 5, 6] else 0)  # X21: dark conditions\n",
    "df['driver_age'] = df[24]   # X25: driver’s age\n",
    "df['male'] = df[25]         # X26: gender (already coded 0/1)\n",
    "df['ejected'] = df[26].apply(lambda x: 1 if x in [2, 3] else 0)  # X27: ejection status\n",
    "df['no_restraint'] = (df[27] == 3).astype(int)  # X28: no restraint used\n",
    "\n",
    "# IMPORTANT: For the segment-based model, we need a segment identifier.\n",
    "# We'll use X3 (Link ID) as the segment identifier.\n",
    "df['segment'] = df[2]\n",
    "\n",
    "# Now, select the variables we need and drop missing values:\n",
    "cols = ['segment', 'severity', 'adverse_weather', 'dark', 'driver_age', \n",
    "        'male', 'ejected', 'no_restraint']\n",
    "df_model = df[cols].dropna()\n",
    "\n",
    "# Define outcome and predictors:\n",
    "y = df_model['severity']\n",
    "X = df_model[['adverse_weather', 'dark', 'driver_age', 'male', 'ejected', 'no_restraint']]\n",
    "\n",
    "# -------------------------------\n",
    "# Step 2: Segment-Based EM Setup\n",
    "# -------------------------------\n",
    "# In a segment-based model, we treat all accidents on a given segment as a group.\n",
    "# We'll compute the responsibilities (posterior probabilities) at the segment level.\n",
    "\n",
    "# Get a list of unique segments and create a mapping: segment -> indices (rows)\n",
    "segments = df_model['segment'].unique()\n",
    "seg_groups = df_model.groupby('segment').indices\n",
    "\n",
    "# For initialization, assign a random responsibility for each segment.\n",
    "# For example, for a 2-class model:\n",
    "np.random.seed(42)\n",
    "K = 2\n",
    "seg_resp = {}  # dictionary mapping segment -> array of responsibilities (length K)\n",
    "for seg in segments:\n",
    "    # Randomly initialize so they are not exactly 0.5\n",
    "    r_val = np.random.uniform(0.4, 0.6)\n",
    "    seg_resp[seg] = np.array([r_val, 1 - r_val])\n",
    "\n",
    "# For individual observations, we later assign each observation the responsibility\n",
    "# of its segment.\n",
    "# Set up convergence parameters:\n",
    "max_iter = 100\n",
    "tol_ll = 1e-4\n",
    "tol_params = 1e-4\n",
    "prev_ll = -np.inf\n",
    "\n",
    "# We'll store class-specific parameters from the weighted ordered probit models:\n",
    "prev_class_params = [None] * K\n",
    "\n",
    "# -------------------------------\n",
    "# Step 3: EM Algorithm (Segment-Based)\n",
    "# -------------------------------\n",
    "# The idea is similar to the accident-based model but with responsibilities updated per segment.\n",
    "\n",
    "for iteration in range(max_iter):\n",
    "    # --- M-step: Fit class-specific ordered probit models using weights from segments ---\n",
    "    # For each observation, assign its weight as the segment’s responsibility.\n",
    "    # Create a weight array of shape (N, K) for each class.\n",
    "    N = len(df_model)\n",
    "    # For class c, weight for observation i is the responsibility of the segment that observation belongs to.\n",
    "    weights_per_class = np.zeros((N, K))\n",
    "    for seg, indices in seg_groups.items():\n",
    "        for c in range(K):\n",
    "            weights_per_class[indices, c] = seg_resp[seg][c]\n",
    "    \n",
    "    # Now, for each class, fit an ordered probit model using the corresponding weights.\n",
    "    class_params = []\n",
    "    class_thresholds = []\n",
    "    for c in range(K):\n",
    "        # Use the weight for class c for each observation\n",
    "        weights = weights_per_class[:, c]\n",
    "        model_c = OrderedModel(y, X, distr='probit')\n",
    "        res_c = model_c.fit(method='bfgs', weights=weights, disp=False)\n",
    "        n_thresholds = model_c.k_levels - 1\n",
    "        params_c = res_c.params\n",
    "        class_params.append(params_c[:-n_thresholds])\n",
    "        class_thresholds.append(params_c[-n_thresholds:])\n",
    "    \n",
    "    # Update mixing proportions at the segment level:\n",
    "    # For each segment, compute the segment likelihood under each class, then update responsibilities.\n",
    "    # Also compute overall log-likelihood.\n",
    "    pi = np.zeros(K)\n",
    "    ll_total = 0\n",
    "    for seg, indices in seg_groups.items():\n",
    "        # For each segment, calculate the product of likelihoods for all observations in that segment,\n",
    "        # separately for each class.\n",
    "        seg_ll = np.zeros(K)\n",
    "        for c in range(K):\n",
    "            seg_ll_c = 0  # we'll work in log space to avoid underflow\n",
    "            for i in indices:\n",
    "                xb = np.dot(X.iloc[i], class_params[c])\n",
    "                # Compute likelihood for observation i given its outcome.\n",
    "                if y.iloc[i] == 0:\n",
    "                    l_val = norm.cdf(class_thresholds[c][0] - xb)\n",
    "                elif y.iloc[i] == 1:\n",
    "                    l_val = norm.cdf(class_thresholds[c][1] - xb) - norm.cdf(class_thresholds[c][0] - xb)\n",
    "                elif y.iloc[i] == 2:\n",
    "                    l_val = norm.cdf(class_thresholds[c][2] - xb) - norm.cdf(class_thresholds[c][1] - xb)\n",
    "                elif y.iloc[i] == 3:\n",
    "                    l_val = 1 - norm.cdf(class_thresholds[c][-1] - xb)\n",
    "                # Avoid zeros by taking max:\n",
    "                l_val = max(l_val, 1e-8)\n",
    "                seg_ll_c += np.log(l_val)\n",
    "            seg_ll[c] = np.exp(seg_ll_c)  # Convert back from log-sum to product-like likelihood\n",
    "        \n",
    "        # Now update the responsibility for this segment.\n",
    "        # Compute weighted likelihoods: pi_c (old mixing proportion) is not yet updated,\n",
    "        # so for the first iteration we assume uniform mixing, or we can use current seg_resp as prior.\n",
    "        # For updating, we use current mixing proportions computed over segments.\n",
    "        # Here, we'll simply assume equal prior if iteration==0, otherwise average from previous seg_resp.\n",
    "        if iteration == 0:\n",
    "            prior = np.ones(K) / K\n",
    "        else:\n",
    "            # For simplicity, average the responsibilities over segments from last iteration\n",
    "            prior = np.array([np.mean([seg_resp[s][c] for s in seg_groups]) for c in range(K)])\n",
    "        weighted_ll = prior * seg_ll\n",
    "        total_weighted = np.sum(weighted_ll)\n",
    "        # Update responsibility for this segment:\n",
    "        new_resp = weighted_ll / total_weighted\n",
    "        seg_resp[seg] = new_resp\n",
    "        # Add to overall log-likelihood (using weighted likelihood for the segment)\n",
    "        ll_total += np.log(total_weighted)\n",
    "    \n",
    "    # Update overall mixing proportions from segments\n",
    "    # For each class, take the average responsibility across segments weighted by number of accidents.\n",
    "    counts = np.array([len(indices) for indices in seg_groups.values()])\n",
    "    weighted_sum = np.zeros(K)\n",
    "    total_obs = 0\n",
    "    for seg, indices in seg_groups.items():\n",
    "        weighted_sum += seg_resp[seg] * len(indices)\n",
    "        total_obs += len(indices)\n",
    "    pi = weighted_sum / total_obs\n",
    "    \n",
    "    # Convergence checks on log-likelihood and parameter changes:\n",
    "    ll_change = np.abs(ll_total - prev_ll)\n",
    "    param_change = 0\n",
    "    if iteration > 0:\n",
    "        for c in range(K):\n",
    "            diff = np.abs(class_params[c] - prev_class_params[c])\n",
    "            param_change = max(param_change, np.max(diff))\n",
    "    \n",
    "    print(f\"Iteration {iteration}: Log-likelihood = {ll_total:.4f}, ll_change = {ll_change:.6f}, param_change = {param_change:.6f}\")\n",
    "    \n",
    "    if ll_change < tol_ll and param_change < tol_params:\n",
    "        print(f\"Converged at iteration {iteration}\")\n",
    "        break\n",
    "    \n",
    "    prev_ll = ll_total\n",
    "    prev_class_params = [cp.copy() for cp in class_params]\n",
    "\n",
    "# -------------------------------\n",
    "# Step 4: Displaying the Results\n",
    "# -------------------------------\n",
    "# Print final segment-based mixing proportions and class-specific parameters.\n",
    "print(\"\\nFinal Mixing Proportions (Segment-Based):\", pi)\n",
    "for c in range(K):\n",
    "    print(f\"\\nClass {c+1} Coefficients:\")\n",
    "    print(class_params[c])\n",
    "    print(f\"Class {c+1} Thresholds:\")\n",
    "    print(class_thresholds[c])\n",
    "\n",
    "# Optionally, assign each segment to a class (the one with highest posterior probability)\n",
    "# and then propagate that to individual observations.\n",
    "segment_assignments = {seg: np.argmax(resp) + 1 for seg, resp in seg_resp.items()}\n",
    "df_model['predicted_class'] = df_model['segment'].map(segment_assignments)\n",
    "print(\"\\nDistribution of Observations by Predicted Segment Class:\")\n",
    "print(df_model['predicted_class'].value_counts())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
